{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (55000, 784)\n",
      "Train labels shape:  (55000,)\n",
      "Test data shape:  (10000, 784)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "\n",
    "print ('Train data shape: ', X_train.shape)\n",
    "print ('Train labels shape: ', y_train.shape)\n",
    "print ('Test data shape: ', X_test.shape)\n",
    "print ('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-728bf8c8e378>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/dnn-train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/dnn-test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "train_writer = tf.summary.FileWriter('/tmp/dnn-train', sess.graph)\n",
    "test_writer = tf.summary.FileWriter('/tmp/dnn-test')\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "n_batches = 50\n",
    "batch_count = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    all_loss = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(batch_count):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            one_loss, summary, _ = sess.run([loss, merged, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "            all_loss.append(one_loss)\n",
    "            train_writer.add_summary(summary, epoch * batch_count + iteration)\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\"Epoch\", epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Loss: \", np.mean(all_loss))\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c6cc149fa4a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_graph' is not defined"
     ]
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train accuracy: 0.88 Test accuracy: 0.8942 Loss:  0.779578\n",
      "Epoch 1 Train accuracy: 0.9 Test accuracy: 0.9102 Loss:  0.582041\n",
      "Epoch 2 Train accuracy: 0.88 Test accuracy: 0.917 Loss:  0.49755\n",
      "Epoch 3 Train accuracy: 0.96 Test accuracy: 0.9242 Loss:  0.44766\n",
      "Epoch 4 Train accuracy: 0.92 Test accuracy: 0.9295 Loss:  0.413232\n",
      "Epoch 5 Train accuracy: 0.9 Test accuracy: 0.9327 Loss:  0.387275\n",
      "Epoch 6 Train accuracy: 0.94 Test accuracy: 0.9356 Loss:  0.366545\n",
      "Epoch 7 Train accuracy: 0.92 Test accuracy: 0.9394 Loss:  0.349351\n",
      "Epoch 8 Train accuracy: 0.88 Test accuracy: 0.942 Loss:  0.334682\n",
      "Epoch 9 Train accuracy: 0.94 Test accuracy: 0.9443 Loss:  0.321893\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Variate hidden layer size\n",
    "'''\n",
    "\n",
    "with tf.name_scope(\"init\"):\n",
    "    n_inputs = 28*28  # MNIST\n",
    "    n_hidden = 300\n",
    "    n_outputs = 10\n",
    "    n_epochs = 10\n",
    "    batch_size = 50\n",
    "    batch_count = mnist.train.num_examples // batch_size\n",
    "\n",
    "reset_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden = tf.layers.dense(X, n_hidden, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "train_writer = tf.summary.FileWriter('/tmp/dnn-train-' + str(n_hidden), sess.graph)\n",
    "\n",
    "init.run()\n",
    "all_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(batch_count):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        one_loss, summary, _ = sess.run([loss, merged, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        all_loss.append(one_loss)\n",
    "        train_writer.add_summary(summary, epoch * batch_count + iteration)\n",
    "    acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Epoch\", epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Loss: \", np.mean(all_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train accuracy: 0.92 Test accuracy: 0.8721 Loss:  1.04761\n",
      "Epoch 1 Train accuracy: 0.88 Test accuracy: 0.8965 Loss:  0.747533\n",
      "Epoch 2 Train accuracy: 0.9 Test accuracy: 0.9065 Loss:  0.623237\n",
      "Epoch 3 Train accuracy: 0.9 Test accuracy: 0.9103 Loss:  0.552652\n",
      "Epoch 4 Train accuracy: 0.96 Test accuracy: 0.9155 Loss:  0.506059\n",
      "Epoch 5 Train accuracy: 0.96 Test accuracy: 0.919 Loss:  0.472424\n",
      "Epoch 6 Train accuracy: 0.84 Test accuracy: 0.9217 Loss:  0.446638\n",
      "Epoch 7 Train accuracy: 0.94 Test accuracy: 0.9225 Loss:  0.426037\n",
      "Epoch 8 Train accuracy: 0.84 Test accuracy: 0.9254 Loss:  0.409013\n",
      "Epoch 9 Train accuracy: 0.92 Test accuracy: 0.9269 Loss:  0.394599\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Variate the number of hidden layers\n",
    "'''\n",
    "\n",
    "with tf.name_scope(\"init\"):\n",
    "    n_inputs = 28*28  # MNIST\n",
    "    n_hidden = 20\n",
    "    n_outputs = 10\n",
    "    n_epochs = 10\n",
    "    batch_size = 50\n",
    "    batch_count = mnist.train.num_examples // batch_size\n",
    "\n",
    "reset_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    '''\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu)\n",
    "    '''\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "train_writer = tf.summary.FileWriter('/tmp/dnn-train-' + str(n_hidden), sess.graph)\n",
    "\n",
    "init.run()\n",
    "all_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(batch_count):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        one_loss, summary, _ = sess.run([loss, merged, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        all_loss.append(one_loss)\n",
    "        train_writer.add_summary(summary, epoch * batch_count + iteration)\n",
    "    acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    print(\"Epoch\", epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Loss: \", np.mean(all_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reduce training set size\n",
    "'''\n",
    "import pylab as pl\n",
    "\n",
    "train_size = 500\n",
    "with tf.name_scope(\"init\"):\n",
    "    n_inputs = 28*28  # MNIST\n",
    "    n_hidden1 = 300\n",
    "    n_hidden2 = 100\n",
    "    n_epochs = 50\n",
    "    batch_size = 50\n",
    "    batch_count = train_size // batch_size\n",
    "    # batch_count = mnist.train.num_examples // batch_size\n",
    "\n",
    "reset_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "init.run()\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for epoch in range(n_epochs):\n",
    "    all_loss = []\n",
    "    for iteration in range(batch_count):\n",
    "        # X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        X_batch, y_batch = mnist.train.images[iteration*batch_size : (iteration+1)*batch_size,], \\\n",
    "                           mnist.train.labels[iteration*batch_size : (iteration+1)*batch_size,]\n",
    "        one_loss, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        all_loss.append(one_loss)\n",
    "    acc_train = accuracy.eval(feed_dict={X: mnist.train.images[:train_size,], y: mnist.train.labels[:train_size,]})\n",
    "    # acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.train.images[train_size:(train_size+1000),],\n",
    "                                        y: mnist.train.labels[train_size:(train_size+1000),]})\n",
    "    train_acc.append(acc_train)\n",
    "    test_acc.append(acc_test)\n",
    "    print(\"Epoch\", epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Loss: \", np.mean(all_loss))\n",
    "    \n",
    "pl.plot(train_acc, label='Train Accuracy')\n",
    "pl.plot(test_acc, label = 'Test Accuracy')\n",
    "pl.xlabel('Epoch')\n",
    "pl.ylabel('Accuracy')\n",
    "pl.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "pl.title(\"Train set size: \" + str(train_size))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train accuracy: 0.6815 Test accuracy: 0.6863 Loss:  2.01894\n",
      "Epoch 1 Train accuracy: 0.809 Test accuracy: 0.8033 Loss:  1.32799\n",
      "Epoch 2 Train accuracy: 0.8375 Test accuracy: 0.8466 Loss:  0.871382\n",
      "Epoch 3 Train accuracy: 0.869 Test accuracy: 0.8691 Loss:  0.652726\n",
      "Epoch 4 Train accuracy: 0.875 Test accuracy: 0.8796 Loss:  0.55067\n",
      "Epoch 5 Train accuracy: 0.8805 Test accuracy: 0.8869 Loss:  0.485395\n",
      "Epoch 6 Train accuracy: 0.9 Test accuracy: 0.8925 Loss:  0.445062\n",
      "Epoch 7 Train accuracy: 0.883 Test accuracy: 0.8998 Loss:  0.411068\n",
      "Epoch 8 Train accuracy: 0.896 Test accuracy: 0.9022 Loss:  0.393546\n",
      "Epoch 9 Train accuracy: 0.896 Test accuracy: 0.9065 Loss:  0.377842\n",
      "Epoch 10 Train accuracy: 0.905 Test accuracy: 0.9077 Loss:  0.356704\n",
      "Epoch 11 Train accuracy: 0.9 Test accuracy: 0.9095 Loss:  0.3479\n",
      "Epoch 12 Train accuracy: 0.9045 Test accuracy: 0.9118 Loss:  0.334539\n",
      "Epoch 13 Train accuracy: 0.909 Test accuracy: 0.914 Loss:  0.32753\n",
      "Epoch 14 Train accuracy: 0.909 Test accuracy: 0.9148 Loss:  0.321307\n",
      "Epoch 15 Train accuracy: 0.92 Test accuracy: 0.9162 Loss:  0.309209\n",
      "Epoch 16 Train accuracy: 0.9115 Test accuracy: 0.9173 Loss:  0.307396\n",
      "Epoch 17 Train accuracy: 0.924 Test accuracy: 0.9199 Loss:  0.296574\n",
      "Epoch 18 Train accuracy: 0.9375 Test accuracy: 0.9206 Loss:  0.292876\n",
      "Epoch 19 Train accuracy: 0.9135 Test accuracy: 0.9224 Loss:  0.29268\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEgCAYAAABxQp66AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e44b790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Use other optimizer\n",
    "'''\n",
    "import pylab as pl\n",
    "\n",
    "with tf.name_scope(\"init\"):\n",
    "    n_inputs = 28*28  # MNIST\n",
    "    n_hidden1 = 300\n",
    "    n_hidden2 = 100\n",
    "    n_epochs = 20\n",
    "    batch_size = 50\n",
    "    batch_count = mnist.train.num_examples // batch_size\n",
    "\n",
    "reset_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") \n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "    # tf.train.MomentumOptimizer(learning_rate = 0.001, momentum = 0.9)\n",
    "    tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "init.run()\n",
    "test_acc = []\n",
    "for epoch in range(n_epochs):\n",
    "    all_loss = []\n",
    "    for iteration in range(batch_count):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        one_loss, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        all_loss.append(one_loss)\n",
    "    acc_train = accuracy.eval(feed_dict={X: mnist.train.images[:train_size,], y: mnist.train.labels[:train_size,]})\n",
    "    acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "    test_acc.append(acc_test)\n",
    "    print(\"Epoch\", epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Loss: \", np.mean(all_loss))\n",
    "    \n",
    "\n",
    "pl.plot(test_acc)\n",
    "pl.xlabel('Epoch')\n",
    "pl.ylabel('Accuracy')\n",
    "pl.title('Test Accuracy: Adam')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "nav_menu": {
   "height": "264px",
   "width": "369px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
